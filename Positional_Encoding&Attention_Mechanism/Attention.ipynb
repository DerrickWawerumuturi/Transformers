{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import required libraries",
   "id": "acb92a87054efa2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:51:59.461269Z",
     "start_time": "2025-07-02T13:51:59.434845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "\n",
    "from Levenshtein import distance # type:ignore\n",
    "from torchtext.data.utils import get_tokenizer # type:ignore\n",
    "from torchtext.vocab import build_vocab_from_iterator # type:ignore\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "83ee873ed116b264",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T14:49:01.841787Z",
     "start_time": "2025-07-02T14:49:01.784523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device for training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "split = 'train'\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "max_iters = 5000  # Maximum training iterations\n",
    "eval_interval = 200 # Evaluate model every 'eval_interval' iterations in the training loop\n",
    "eval_iters = 100 # When evaluating, approximate loss using 'eval_iters' batches\n",
    "\n",
    "# Architecture parameters\n",
    "max_vocab_size = 256          # Maximum vocabulary size\n",
    "vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
    "block_size = 16               # Context length for predictions\n",
    "n_embd = 32                   # Embedding size\n",
    "num_heads = 2                 # Number of head in multi-headed attention\n",
    "n_layer = 2                   # Number of Blocks\n",
    "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
    "dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n",
    "\n",
    "head_size = n_embd // num_heads\n",
    "assert (num_heads * head_size) == n_embd\n"
   ],
   "id": "946d196f0d197079",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:10:41.169110Z",
     "start_time": "2025-07-02T08:10:41.133815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_embdings(my_embdings,name,vocab):\n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "  # Plot the data points\n",
    "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
    "\n",
    "  # Label the points\n",
    "  for j, label in enumerate(name):\n",
    "      i=vocab.get_stoi()[label]\n",
    "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
    "\n",
    "  # Set axis labels\n",
    "  ax.set_xlabel('X Label')\n",
    "  ax.set_ylabel('Y Label')\n",
    "  ax.set_zlabel('Z Label')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ],
   "id": "747f4e0ae7786f28",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:52:56.403436Z",
     "start_time": "2025-07-02T13:52:56.339970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import Levenshtein # type:ignore\n",
    "# Levenshtein works by counting the minimum number of single-character edits need to turn one string into another\n",
    "\n",
    "dis = Levenshtein.distance(\"tabel\", 'under')\n",
    "print(dis)"
   ],
   "id": "cdbe2b0e1bd333eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Program for literal translation\n",
    "\n",
    "In the next part, let's explore the fundamental concepts of tokenization and translation through a simple program for literal translation from French to English:\n",
    "\n",
    "- A `dictionary` is defined, mapping French words to their English equivalents, forming the basis of our translation logic.\n"
   ],
   "id": "96c4ad70065d994c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T12:53:40.043881Z",
     "start_time": "2025-07-01T12:53:40.033421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dictionary = {\n",
    "    'le': 'the'\n",
    "    , 'chat': 'cat'\n",
    "    , 'est': 'is'\n",
    "    , 'sous': 'under'\n",
    "    , 'la': 'the'\n",
    "    , 'table': 'table'\n",
    "}"
   ],
   "id": "9526aa9908db14ad",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The `tokenize` function is responsible for breaking down a sentence into individual words.\n",
    "- The `translate` function uses this `tokenize` function to split the input sentence and then translates each word according to the dictionary. The translated words are concatenated to form the output sentence.\n"
   ],
   "id": "90e5fccbc48218e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T12:53:42.212039Z",
     "start_time": "2025-07-01T12:53:42.196349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to split a sentence into tokens (words)\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function takes a string of text as input and returns a list of words (tokens).\n",
    "    It uses the split method, which by default splits on any whitespace, to tokenize the text.\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "# Function to translate a sentence from source to target language word by word\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    This function translates a sentence by looking up each word's translation in a predefined dictionary.\n",
    "    It assumes that every word in the sentence is a key in the dictionary.\n",
    "    \"\"\"\n",
    "    out = '' # Initialize the output string\n",
    "    for token in tokenize(sentence): # Tokenize the sentence into words\n",
    "        # Append the translated word to the output string\n",
    "        # This line assumes the dictionary contains a translation for every word in the input\n",
    "        out += dictionary[token] + ' '\n",
    "    return out.strip() # Return the translated sentence, stripping any extra whitespace"
   ],
   "id": "2db6025af08807f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T12:54:09.322818Z",
     "start_time": "2025-07-01T12:54:09.301474Z"
    }
   },
   "cell_type": "code",
   "source": "translate('le chat est sous la table')",
   "id": "90a9ffdac7a07032",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Improvement: What if the 'key' is not in the dictionary?\n",
    "\n",
    "The code presents an enhancement to the translation program, addressing the scenario when a word does not exist in our dictionary:\n",
    "\n",
    "- **find_closest_key Function**: This new function aims to find the closest key in the dictionary to a given query word. It uses the **Levenshtein distance** (a measure of the difference between two sequences) to find the dictionary key with the minimum distance to the query, suggesting a similar word if an exact match isn't found.\n",
    "\n",
    "- **Improved translate function**: The `translate` function is updated to use `find_closest_key`. Now, instead of directly translating tokens based on the dictionary, it first finds the closest key for each tokenized word. This allows for a more robust translation, especially when encountering words with minor spelling errors or variations not present in the dictionary.\n",
    "\n",
    "- **Demonstration**: The improved translate function is demonstrated with the input \"tables\". Although \"tables\" is not in the dictionary, the function is expected to find and use the closest key \"table\" for the translation, outputting \"table\" in English.\n",
    "\n",
    "This improvement showcases a simple form of error handling and fuzzy matching in translation systems, allowing for more flexible and fault-tolerant translations.\n"
   ],
   "id": "7612e3d3bffc1e72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T13:03:38.469633Z",
     "start_time": "2025-07-01T13:03:38.434538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to find the closest key in the dictionary to the given query word\n",
    "def find_closest_key(query):\n",
    "    \"\"\"\n",
    "    The function computes the Levenshtein distance between the query and each key in the dictionary.\n",
    "    The Levenshtein distance is a measure of the number of single-character edits required to change one word into the other.\n",
    "    \"\"\"\n",
    "    closest_key, min_dist = None, float('inf')  # Initialize the closest key and minimum distance to infinity\n",
    "    for key in dictionary.keys():\n",
    "        dist = distance(query, key)  # Calculate the Levenshtein distance to the current key\n",
    "        if dist < min_dist:  # If the current distance is less than the previously found minimum\n",
    "            min_dist, closest_key = dist, key  # Update the minimum distance and the closest key\n",
    "    return closest_key  # Return the closest key found\n",
    "\n",
    "# Function to translate a sentence from source to target language using the dictionary\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input sentence into words and finds the closest translation for each word.\n",
    "    It constructs the translated sentence by appending the translated words together.\n",
    "    \"\"\"\n",
    "    out = ''  # Initialize the output string\n",
    "    for query in tokenize(sentence):  # Tokenize the sentence into words\n",
    "        key = find_closest_key(query)  # Find the closest key in the dictionary for each word\n",
    "        out += dictionary[key] + ' '  # Append the translation of the closest key to the output string\n",
    "    return out.strip()  # Return the translated sentence, stripping any extra whitespace"
   ],
   "id": "22487b5f599ef59",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T13:03:41.364212Z",
     "start_time": "2025-07-01T13:03:41.332865Z"
    }
   },
   "cell_type": "code",
   "source": "translate('tables')",
   "id": "6a7354598de7678a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'table'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Convert to neural network\n",
    "\n",
    "Transitioning from basic translation to neural networks, let's start by defining our input and output vocabularies and then move on to encoding our tokens:\n",
    "\n",
    "- **Vocabulary definition**: Two vocabularies are created from the dictionaryâ€”`vocabulary_in` for the source language (French) and `vocabulary_out` for the target language (English). These vocabularies are the lists of unique words obtained from the dictionary's keys and values, respectively, and they are sorted to maintain a consistent order.\n",
    "\n",
    "- **One-hot encoding**: The `encode_one_hot` function is introduced to convert each word in the vocabulary into a one-hot encoded vector. One-hot encoding is a process where represents each word as a binary vector with a '1' in the position corresponding to the word's index in the vocabulary and '0's elsewhere. This creates a unique, fixed-size vector for each word, which is essential for neural network processing.\n",
    "\n",
    "- **Encoding demonstration**: Demonstrate the one-hot encoding process by applying `encode_one_hot` to our input vocabulary (`vocabulary_in`) and showing the encoded vectors for each word. The same process is then applied to the output vocabulary (`vocabulary_out`).\n",
    "\n",
    "This step is critical in machine learning as it prepares our textual data for input into a neural network, allowing it to learn from and make predictions on our data.\n"
   ],
   "id": "ae7e76f857f15076"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define 'vocabularies'\n",
   "id": "e9d6c0e960fb735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T13:11:52.254424Z",
     "start_time": "2025-07-01T13:11:52.226915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create and sort the input vocabulary from the dictionary's keys\n",
    "vocabulary_in = sorted(list(set(dictionary.keys())))\n",
    "# Display the size and the sorted vocabulary for the input language\n",
    "print(f\"Vocabulary input ({len(vocabulary_in)}): {vocabulary_in}\")\n",
    "\n",
    "# Create and sort the output vocabulary from the dictionary's values\n",
    "vocabulary_out = sorted(list(set(dictionary.values())))\n",
    "# Display the size and the sorted vocabulary for the output language\n",
    "print(f\"Vocabulary output ({len(vocabulary_out)}): {vocabulary_out}\")"
   ],
   "id": "2b341ddcb18c70f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary input (6): ['chat', 'est', 'la', 'le', 'sous', 'table']\n",
      "Vocabulary output (5): ['cat', 'is', 'table', 'the', 'under']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Encode tokens using 'one hot' encoding\n",
   "id": "f247f87144ae3e58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:12:27.316770Z",
     "start_time": "2025-07-02T08:12:27.293948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to convert a list of vocabulary words into one-hot encoded vectors\n",
    "def encode_one_hot(vocabulary):\n",
    "    vocabulary_size = len(vocabulary)  # Get the size of the vocabulary\n",
    "    one_hot = dict()  # Initialize a dictionary to hold our one-hot encodings\n",
    "    LEN = len(vocabulary)  # The length of each one-hot encoded vector will be equal to the vocabulary size\n",
    "\n",
    "    # Iterate over the vocabulary to create a one-hot encoded vector for each word\n",
    "    for i, key in enumerate(vocabulary):\n",
    "        one_hot_vector = torch.zeros(LEN)  # Start with a vector of zeros\n",
    "        one_hot_vector[i] = 1  # Set the i-th position to 1 for the current word\n",
    "        one_hot[key] = one_hot_vector  # Map the word to its one-hot encoded vector\n",
    "        print(f\"{key}\\t: {one_hot[key]}\")  # Print each word and its encoded vector\n",
    "\n",
    "    return one_hot  # Return the dictionary of words and their one-hot encoded vectors"
   ],
   "id": "b21b1edc82560382",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:12:32.662564Z",
     "start_time": "2025-07-02T08:12:32.643255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the one-hot encoding function to the input vocabulary and store the result\n",
    "one_hot_in = encode_one_hot(vocabulary_in)"
   ],
   "id": "7fe68a391d18f550",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat\t: tensor([1., 0., 0., 0., 0., 0.])\n",
      "est\t: tensor([0., 1., 0., 0., 0., 0.])\n",
      "la\t: tensor([0., 0., 1., 0., 0., 0.])\n",
      "le\t: tensor([0., 0., 0., 1., 0., 0.])\n",
      "sous\t: tensor([0., 0., 0., 0., 1., 0.])\n",
      "table\t: tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:12:37.410227Z",
     "start_time": "2025-07-02T08:12:37.390224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the one-hot encoding function to the output vocabulary and store the result\n",
    "# This time we're encoding the target language vocabulary\n",
    "one_hot_out = encode_one_hot(vocabulary_out)"
   ],
   "id": "7d0ba40c62f5f22c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\t: tensor([1., 0., 0., 0., 0.])\n",
      "is\t: tensor([0., 1., 0., 0., 0.])\n",
      "table\t: tensor([0., 0., 1., 0., 0.])\n",
      "the\t: tensor([0., 0., 0., 1., 0.])\n",
      "under\t: tensor([0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:12:39.479511Z",
     "start_time": "2025-07-02T08:12:39.454176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Iterate over the one-hot encoded input vocabulary and print each vector\n",
    "# This visualizes the one-hot representation for each word in the input vocabulary\n",
    "for k, v in one_hot_in.items():\n",
    "    print(f\"E_{{ {k} }} = \" , v)"
   ],
   "id": "20c61daf28b2a619",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_{ chat } =  tensor([1., 0., 0., 0., 0., 0.])\n",
      "E_{ est } =  tensor([0., 1., 0., 0., 0., 0.])\n",
      "E_{ la } =  tensor([0., 0., 1., 0., 0., 0.])\n",
      "E_{ le } =  tensor([0., 0., 0., 1., 0., 0.])\n",
      "E_{ sous } =  tensor([0., 0., 0., 0., 1., 0.])\n",
      "E_{ table } =  tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Let's create a 'dictionary' using matrix multiplication\n",
    "\n",
    "We're now illustrating how to create a representation of our dictionary suitable for neural network operations:\n",
    "\n",
    "- **Matrix creation**: Using PyTorch's `torch.stack`, convert the one-hot encoded vectors for both input (`K`) and output (`V`) vocabularies into tensors. `K` is constructed from the input vocabulary's one-hot vectors, and `V` from the output vocabulary's vectors. These tensors can be thought of as a look-up table that our model will use to associate input tokens with output tokens.\n",
    "\n",
    "- **Dictionary as matrices**: This step effectively translates our word-to-word dictionary mapping into a neural network-friendly format. Each row in `K` corresponds to a word in the input language represented as a one-hot vector, and each row in `V` corresponds to the respective translated word in the output language.\n",
    "\n",
    "- **Query example**: An example shows how to use matrix operations to find a translation. Look up the one-hot vector for the word \"sous\" from the input vocabulary (`q`). Then demonstrate how to find its corresponding translation by performing matrix multiplication with the transpose of `K` (i.e., `q @ K.T`) to identify the index and then use that index to select the relevant row from `V`. This process mimics the lookup the you would perform in an actual neural network during translation tasks.\n",
    "\n",
    "This matrix representation is a precursor to understanding how more complex neural network architectures, like those using self-attention, manage token translations.\n"
   ],
   "id": "c0dc144d1cd62069"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:12:44.216210Z",
     "start_time": "2025-07-02T08:12:44.194169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stacking the one-hot encoded vectors for input vocabulary to form a tensor\n",
    "K = torch.stack([one_hot_in[k] for k in dictionary.keys()])\n",
    "# K now represents a matrix of one-hot vectors for the input vocabulary\n",
    "\n",
    "# Display the tensor for verification\n",
    "print(K)"
   ],
   "id": "fcd4627b84c1b07e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:12:46.466355Z",
     "start_time": "2025-07-02T08:12:46.451863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Similarly, stack the one-hot encoded vectors for output vocabulary to form a tensor\n",
    "V = torch.stack([one_hot_out[k] for k in dictionary.values()])\n",
    "# V represents the corresponding matrix of one-hot vectors for the output vocabulary\n",
    "\n",
    "# Display the tensor for verification\n",
    "print(V)"
   ],
   "id": "43183f1bcb81dc4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:12:49.232445Z",
     "start_time": "2025-07-02T08:12:49.217643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demonstrating how to look up a translation for a given word using matrix operations\n",
    "# Here, we take the one-hot representation of 'sous' from the input vocabulary\n",
    "q = one_hot_in['sous']\n",
    "# Display the query token vector\n",
    "print(\"Query token :\", q)"
   ],
   "id": "b5fe7278d950c41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query token : tensor([0., 0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:12:50.737655Z",
     "start_time": "2025-07-02T08:12:50.719114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select the corresponding key vector in K (input dictionary matrix) using matrix multiplication\n",
    "# This operation gives us the index where 'sous' would be '1' in the one-hot encoded input matrix\n",
    "print(\"Select key (K) :\", q @ K.T)"
   ],
   "id": "d8dc57e2baf9d2ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select key (K) : tensor([0., 0., 0., 1., 0., 0.])\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:12:53.477218Z",
     "start_time": "2025-07-02T08:12:53.465012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use the index found from the key selection to find the corresponding value vector in V (output dictionary matrix)\n",
    "# This operation selects the row from V that is the translation of 'sous' in the output vocabulary\n",
    "print(\"Select value (V):\", q @ K.T @ V)\n",
    "\n",
    "# The final output demonstrates how 'sous' can be translated using the neural network approach"
   ],
   "id": "52f457327718f63b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select value (V): tensor([0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Query vector, K matrix, and V matrix:\n",
    "\n",
    "$$\n",
    "q = \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 0 & 1 & 0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "; \\\n",
    "K = \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 1 & 0 & 0\\\\\\\\\n",
    "  1 & 0 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 1 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  0 & 0 & 1 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 1\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "; \\\n",
    "V = \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  1 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 1 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 1\\\\\\\\\n",
    "  0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  0 & 0 & 1 & 0 & 0\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n"
   ],
   "id": "65afbd7a7f182f55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "q \\cdot K^T \\cdot V =\n",
    "%\\hspace{2cm}\n",
    "\\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 1 & 0 & 0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "%\\hspace{2.5cm}\n",
    "\\cdot\n",
    "\\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  1 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 1 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 1\\\\\\\\\n",
    "  0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  0 & 0 & 1 & 0 & 0\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "\\hspace{4.5cm}\n",
    "$$\n"
   ],
   "id": "66cf8c55c2f7e094"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "q \\cdot K^T \\cdot V\n",
    "=\n",
    "%\\hspace{3.5cm}\n",
    "\\left[\\begin{matrix}\n",
    "0 & 0 & 0 & 0 & 1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "%\\hspace{3.5cm}\n",
    "\\hspace{9cm}\n",
    "$$\n"
   ],
   "id": "9e1a1429ac008027"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The code introduces a function for decoding one-hot vectors to tokens and updates the translation function to utilize matrix multiplication:\n",
    "\n",
    "### Decode one-hot vector\n",
    "The `decode_one_hot` function is designed to decode a one-hot encoded vector back into the corresponding token (word). It does this by finding the token whose one-hot representation has the highest cosine similarity with the given vector, which is effectively just the dot product due to the nature of one-hot vectors.\n"
   ],
   "id": "242e1089341eb508"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T13:25:52.060414Z",
     "start_time": "2025-07-01T13:25:52.044193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def decode_one_hot(one_hot, vector):\n",
    "    \"\"\"\n",
    "    Decode a one-hot encoded vector to find the best matching token in the vocabulary.\n",
    "    \"\"\"\n",
    "    best_key, best_cosine_sim = None, 0\n",
    "    for k, v in one_hot.items():  # Iterate over the one-hot encoded vocabulary\n",
    "        cosine_sim = torch.dot(vector, v)  # Calculate dot product (cosine similarity)\n",
    "        if cosine_sim > best_cosine_sim:  # If this is the best similarity we've found\n",
    "            best_cosine_sim, best_key = cosine_sim, k  # Update the best similarity and token\n",
    "    return best_key  # Return the token corresponding to the one-hot vector"
   ],
   "id": "aa1dca616c712f61",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T13:27:30.496075Z",
     "start_time": "2025-07-01T13:27:30.484733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    Translate a sentence using matrix multiplication, treating the dictionaries as matrices.\n",
    "    \"\"\"\n",
    "    sentence_out = ''  # Initialize the output sentence\n",
    "    for token_in in tokenize(sentence):  # Tokenize the input sentence\n",
    "        q = one_hot_in[token_in]  # Find the one-hot vector for the token\n",
    "        out = q @ K.T @ V  # Multiply with the input and output matrices to find the translation\n",
    "        token_out = decode_one_hot(one_hot_out, out)  # Decode the output one-hot vector to a token\n",
    "        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n",
    "    return sentence_out.strip()  # Return the translated sentence"
   ],
   "id": "19de79062dc2c676",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Translation test\n",
    "The improved translate function is tested with the sentence \"le chat est sous la table\", verifying that it correctly translates to \"the cat is under the table\" using the matrix operations for a seamless word-by-word translation.\n"
   ],
   "id": "b23adb951d43f6e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:16:22.312204Z",
     "start_time": "2025-07-02T08:16:22.288915Z"
    }
   },
   "cell_type": "code",
   "source": "translate(\"le chat est sous la table\")",
   "id": "ca8d5616fc4bc60d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
